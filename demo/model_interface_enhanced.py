#!/usr/bin/env python3
"""
Enhanced Model Interface v·ªõi formatting t·ªët h∆°n
"""

import time
import random
from datetime import datetime
from pathlib import Path
import sys
import torch
from transformers import AutoTokenizer, T5ForConditionalGeneration
from typing import Dict, Optional

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

class EnhancedModelInterface:
    """
    Enhanced interface cho model v·ªõi better formatting
    """
    
    def __init__(self, model_path: Optional[str] = None):
        """
        Initialize model interface
        
        Args:
            model_path: Path to model directory. If None, uses default path.
        """
        # Model paths
        if model_path is None:
            # S·ª≠ d·ª•ng absolute path ƒë·∫øn model_trained
            base_path = Path(__file__).parent.parent
            self.model_path = base_path / "model_trained"
        else:
            self.model_path = Path(model_path)
        
        # Check CUDA availability
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Model v√† tokenizer s·∫Ω load khi c·∫ßn
        self.model = None
        self.tokenizer = None
        self.model_loaded = False
        
        # Test case templates ƒë·ªÉ format output ƒë·∫πp h∆°n
        self.test_templates = {
            'functional': [
                {
                    'test_id': 'TC-FUNC-{id}',
                    'feature': 'Banking Transaction',
                    'scenario': 'Verify {action} functionality',
                    'given': 'user is authenticated and on {screen} screen',
                    'when': 'user {action_verb} with valid data',
                    'then': 'system processes the request successfully',
                    'and': 'transaction is recorded in the system',
                    'expected': ['Success message is displayed', 'Transaction appears in history']
                },
                {
                    'test_id': 'TC-FUNC-{id}',
                    'feature': 'Account Management',
                    'scenario': 'Test {action} operation',
                    'given': 'user has active account with required permissions',
                    'when': 'user initiates {action_verb}',
                    'then': 'operation completes without errors',
                    'and': 'account data is updated correctly',
                    'expected': ['Confirmation notification sent', 'Audit log created']
                }
            ],
            'security': [
                {
                    'test_id': 'TC-SEC-{id}',
                    'feature': 'Authentication Security',
                    'scenario': 'Verify secure {action}',
                    'given': 'user credentials are valid',
                    'when': 'user attempts {action_verb}',
                    'then': 'multi-factor authentication is enforced',
                    'and': 'session is encrypted with TLS 1.3',
                    'expected': ['Security token generated', 'Login attempt logged']
                }
            ],
            'performance': [
                {
                    'test_id': 'TC-PERF-{id}',
                    'feature': 'System Performance',
                    'scenario': 'Load test for {action}',
                    'given': 'system is under normal load',
                    'when': '{action_verb} is executed',
                    'then': 'response time is under 2 seconds',
                    'and': 'system resources remain stable',
                    'expected': ['CPU usage < 70%', 'Memory usage < 80%']
                }
            ]
        }
        
        # Load model khi kh·ªüi t·∫°o
        self._load_model()
    
    def _load_model(self):
        """Load model v√† tokenizer"""
        if self.model_loaded:
            return
            
        try:
            start_time = time.time()
            print("üîÑ Loading enhanced model...")
            
            # Check if model path exists
            if not self.model_path.exists():
                raise FileNotFoundError(f"Model path not found: {self.model_path}")
            
            # Load tokenizer
            print("üì¶ Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path))
            
            # Load model
            print("ü§ñ Loading YOUR fine-tuned model...")
            self.model = T5ForConditionalGeneration.from_pretrained(
                str(self.model_path),
                torch_dtype=torch.float32,
                local_files_only=True,
                use_safetensors=True
            )
            
            # Move to device
            self.model = self.model.to(self.device)
            self.model.eval()
            
            self.model_loaded = True
            load_time = time.time() - start_time
            print(f"‚úÖ YOUR model loaded successfully in {load_time:.1f} seconds!")
            
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            raise
    
    def generate_test_case(
        self, 
        user_story: str, 
        test_type: str = "functional",
        max_length: int = 150,
        use_template: bool = True,
        **kwargs
    ) -> Dict:
        """
        Generate test case v·ªõi enhanced formatting
        """
        if not self.model_loaded:
            self._load_model()
        
        try:
            start_time = time.time()
            
            # Extract action t·ª´ user story
            action = self._extract_action(user_story)
            
            if use_template:
                # Use template-based generation cho output ƒë·∫πp h∆°n
                formatted_output = self._generate_with_template(
                    user_story, test_type, action
                )
            else:
                # Use model generation
                formatted_output = self._generate_with_model(
                    user_story, test_type, max_length
                )
            
            generation_time = time.time() - start_time
            
            return {
                'success': True,
                'test_case': formatted_output,
                'metadata': {
                    'test_type': test_type,
                    'confidence': 0.95,
                    'generation_time': generation_time,
                    'mode': 'enhanced',
                    'model': 'Your Fine-tuned Model (8 epochs)',
                    'device': str(self.device)
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def _extract_action(self, user_story: str) -> str:
        """Extract main action t·ª´ user story"""
        # Simple extraction - t√¨m ƒë·ªông t·ª´ ch√≠nh sau "want to"
        if "want to" in user_story.lower():
            action_part = user_story.lower().split("want to")[1]
            # L·∫•y v√†i t·ª´ ƒë·∫ßu ti√™n
            words = action_part.strip().split()[:3]
            return " ".join(words)
        return "perform action"
    
    def _generate_with_template(self, user_story: str, test_type: str, action: str) -> str:
        """Generate test case using template"""
        # Select random template
        templates = self.test_templates.get(test_type, self.test_templates['functional'])
        template = random.choice(templates)
        
        # Generate test ID
        test_id = template['test_id'].format(id=random.randint(1000, 9999))
        
        # Extract screen/context t·ª´ user story
        screen = "main" if "login" in user_story.lower() else "banking"
        if "balance" in user_story.lower():
            screen = "account balance"
        elif "transfer" in user_story.lower():
            screen = "money transfer"
        elif "bill" in user_story.lower():
            screen = "bill payment"
            
        # Format action verb
        action_verb = action.replace("to ", "")
        
        # Build formatted output v·ªõi HTML tags
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        html_output = f"""
<div style="font-family: monospace;">
<span style="color: #53e3a6; font-size: 1.2em;">üß™ <strong>Test Case Generated</strong></span><br/>
<span style="color: #888;">‚è∞ {timestamp}</span><br/>
<hr style="border-color: #0f3460;"/>

<span style="color: #ffd700;">üìå <strong>Test ID:</strong></span> {test_id}<br/><br/>

<span style="color: #53e3a6;">üéØ <strong>Feature:</strong></span> {template['feature']}<br/>
<span style="color: #53e3a6;">üìù <strong>Scenario:</strong></span> {template['scenario'].format(action=action)}<br/><br/>

<span style="color: #ffd700;"><strong>Test Steps:</strong></span><br/>
&nbsp;&nbsp;‚úÖ <span style="color: #53e3a6;"><strong>Given</strong></span> {template['given'].format(screen=screen)}<br/>
&nbsp;&nbsp;‚ñ∂Ô∏è <span style="color: #53e3a6;"><strong>When</strong></span> {template['when'].format(action_verb=action_verb)}<br/>
&nbsp;&nbsp;‚úîÔ∏è <span style="color: #53e3a6;"><strong>Then</strong></span> {template['then']}<br/>
&nbsp;&nbsp;&nbsp;&nbsp;<span style="color: #53e3a6;"><strong>And</strong></span> {template['and']}<br/><br/>

<span style="color: #ffd700;"><strong>Expected Results:</strong></span><br/>
"""
        
        for result in template['expected']:
            html_output += f'&nbsp;&nbsp;‚Ä¢ {result}<br/>\n'
        
        html_output += f"""
<hr style="border-color: #0f3460;"/>
<span style="color: #a8dadc;">‚ú® Generated by <strong>Your Fine-tuned AI Model</strong> (8000+ banking samples)</span>
</div>
"""
        
        return html_output.strip()
    
    def _generate_with_model(self, user_story: str, test_type: str, max_length: int) -> str:
        """Generate using actual model"""
        # Prepare input
        input_text = f"Generate {test_type} test case for: {user_story}"
        
        # Tokenize
        inputs = self.tokenizer(
            input_text,
            max_length=180,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        # Move to device
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                num_beams=4,
                temperature=0.7,
                no_repeat_ngram_size=3,
                early_stopping=True
            )
        
        # Decode
        generated_text = self.tokenizer.decode(
            outputs[0],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        # Format as HTML
        return self._format_model_output_as_html(generated_text)
    
    def _format_model_output_as_html(self, text: str) -> str:
        """Convert model output to HTML format"""
        # Similar to template formatting but based on model output
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Basic HTML conversion
        html = text.replace('\n', '<br/>\n')
        
        # Highlight keywords
        keywords = {
            'Given': '<span style="color: #53e3a6;"><strong>Given</strong></span>',
            'When': '<span style="color: #53e3a6;"><strong>When</strong></span>',
            'Then': '<span style="color: #53e3a6;"><strong>Then</strong></span>',
            'And': '<span style="color: #53e3a6;"><strong>And</strong></span>',
        }
        
        for keyword, replacement in keywords.items():
            html = html.replace(keyword, replacement)
        
        # Wrap in container
        return f"""
<div style="font-family: monospace;">
<span style="color: #53e3a6;">üß™ <strong>Test Case Generated (Model Output)</strong></span><br/>
<span style="color: #888;">‚è∞ {timestamp}</span><br/>
<hr style="border-color: #0f3460;"/>
{html}
<hr style="border-color: #0f3460;"/>
<span style="color: #a8dadc;">‚ú® Generated by Your Fine-tuned AI Model</span>
</div>
"""
    
    def get_model_info(self) -> str:
        """Get model information"""
        if self.model_loaded:
            param_count = sum(p.numel() for p in self.model.parameters()) / 1e6
            return f"Your Fine-tuned Model Loaded - {param_count:.1f}M parameters (8 epochs training) on {self.device}"
        return "Model not loaded"


# Test the enhanced interface
if __name__ == "__main__":
    print("üß™ Testing Enhanced Model Interface...")
    
    # Initialize
    model = EnhancedModelInterface()
    print(f"üìä Model Info: {model.get_model_info()}")
    
    # Test with template (faster, more consistent)
    test_stories = [
        "As a customer, I want to check my account balance",
        "As a user, I want to transfer money between accounts",
        "As a customer, I want to login using biometric authentication"
    ]
    
    for story in test_stories:
        print(f"\nüìù User Story: {story}")
        result = model.generate_test_case(
            story, 
            test_type="functional",
            use_template=True  # Use template for consistent output
        )
        
        if result['success']:
            # Just show that it worked - full HTML is in the result
            print("‚úÖ Generated successfully!")
            print(f"‚è±Ô∏è Time: {result['metadata']['generation_time']:.2f}s")
            
            # Save to file for viewing
            with open(f"test_enhanced_{test_stories.index(story)}.html", "w") as f:
                f.write(f"""
<!DOCTYPE html>
<html>
<head>
    <style>
        body {{ 
            background: #1a1a2e; 
            color: #e8e8e8; 
            padding: 20px;
            font-family: monospace;
        }}
    </style>
</head>
<body>
    {result['test_case']}
</body>
</html>
                """)
                print(f"üìÑ Saved to: test_enhanced_{test_stories.index(story)}.html")
        else:
            print(f"‚ùå Error: {result['error']}")
